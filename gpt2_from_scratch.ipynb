{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca5c0917",
   "metadata": {},
   "source": [
    "# Replicating GPT-2 (124M) From Scratch\n",
    "\n",
    "This notebook implements GPT-2 (124M parameters) from scratch in PyTorch, following Andrej Karpathy's approach.\n",
    "\n",
    "**Reference**: [Let's reproduce GPT-2 (124M)](https://www.youtube.com/watch?v=l8pRSuU81PU)\n",
    "\n",
    "## Overview\n",
    "\n",
    "We'll build every component of GPT-2:\n",
    "1. Environment setup and dependencies\n",
    "2. Model configuration\n",
    "3. Multi-head self-attention\n",
    "4. MLP feedforward blocks\n",
    "5. Transformer blocks\n",
    "6. Complete GPT-2 model\n",
    "7. Weight loading from Hugging Face\n",
    "8. Text generation with sampling\n",
    "9. Training from scratch\n",
    "10. Performance optimizations\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf6ef98",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies\n",
    "\n",
    "First, let's install and import all required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a58fe59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/codespace/.local/lib/python3.12/site-packages (2.7.1+cpu)\n",
      "Collecting transformers\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting tiktoken\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.12/site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "  Downloading tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.12/site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.10.22-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (2.32.4)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.10.22-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (2.32.4)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (2025.7.9)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (2025.7.9)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/12.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.10.22-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.4/803.4 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.10.22-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.4/803.4 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, safetensors, regex, hf-xet, tiktoken, huggingface-hub, tokenizers, transformers\n",
      "\u001b[?25lInstalling collected packages: tqdm, safetensors, regex, hf-xet, tiktoken, huggingface-hub, tokenizers, transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8/8\u001b[0m [transformers][0m [transformers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed hf-xet-1.1.10 huggingface-hub-0.35.3 regex-2025.10.22 safetensors-0.6.2 tiktoken-0.12.0 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.57.1\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8/8\u001b[0m [transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed hf-xet-1.1.10 huggingface-hub-0.35.3 regex-2025.10.22 safetensors-0.6.2 tiktoken-0.12.0 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.57.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "!pip install torch transformers tiktoken numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28b4919f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from transformers import GPT2LMHeadModel\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b1e9f0",
   "metadata": {},
   "source": [
    "## 2. Model Configuration Class\n",
    "\n",
    "Define the GPT-2 124M configuration with all hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b27a8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 124M Configuration:\n",
      "  Layers: 12\n",
      "  Hidden size: 768\n",
      "  Attention heads: 12\n",
      "  Vocab size: 50257\n",
      "  Max sequence length: 1024\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    \"\"\"Configuration class for GPT-2 124M model\"\"\"\n",
    "    block_size: int = 1024  # Maximum sequence length\n",
    "    vocab_size: int = 50257  # GPT-2 vocabulary size (50000 BPE merges + 256 bytes tokens + 1 <|endoftext|>)\n",
    "    n_layer: int = 12  # Number of transformer blocks\n",
    "    n_head: int = 12  # Number of attention heads\n",
    "    n_embd: int = 768  # Embedding dimension\n",
    "    dropout: float = 0.0  # Dropout probability (0.1 for training, 0.0 for inference)\n",
    "    bias: bool = True  # Use bias in linear layers and LayerNorm\n",
    "\n",
    "config = GPTConfig()\n",
    "print(f\"GPT-2 124M Configuration:\")\n",
    "print(f\"  Layers: {config.n_layer}\")\n",
    "print(f\"  Hidden size: {config.n_embd}\")\n",
    "print(f\"  Attention heads: {config.n_head}\")\n",
    "print(f\"  Vocab size: {config.vocab_size}\")\n",
    "print(f\"  Max sequence length: {config.block_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca5dd2d",
   "metadata": {},
   "source": [
    "## 3. Tokenizer Setup with Tiktoken\n",
    "\n",
    "Initialize the GPT-2 tokenizer using tiktoken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd189b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Hello, I'm a language model\n",
      "Tokens: [15496, 11, 314, 1101, 257, 3303, 2746]\n",
      "Decoded: Hello, I'm a language model\n",
      "Number of tokens: 7\n"
     ]
    }
   ],
   "source": [
    "# Initialize GPT-2 tokenizer\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "def encode(text):\n",
    "    \"\"\"Encode text to token indices\"\"\"\n",
    "    return enc.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "def decode(tokens):\n",
    "    \"\"\"Decode token indices to text\"\"\"\n",
    "    return enc.decode(tokens)\n",
    "\n",
    "# Test the tokenizer\n",
    "test_text = \"Hello, I'm a language model\"\n",
    "tokens = encode(test_text)\n",
    "decoded = decode(tokens)\n",
    "print(f\"Original: {test_text}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Decoded: {decoded}\")\n",
    "print(f\"Number of tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbf553a",
   "metadata": {},
   "source": [
    "## 4. Multi-Head Self-Attention Implementation\n",
    "\n",
    "Implement the core attention mechanism with causal masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaaf71b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention input shape: torch.Size([2, 10, 768])\n",
      "Attention output shape: torch.Size([2, 10, 768])\n",
      "Number of parameters: 2,362,368\n"
     ]
    }
   ],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head causal self-attention module.\n",
    "    Implements the attention mechanism from \"Attention is All You Need\"\n",
    "    with causal masking for autoregressive generation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        \n",
    "        # Key, Query, Value projections for all heads (in batch)\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        \n",
    "        # Output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        \n",
    "        # Regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        \n",
    "        # Causal mask to ensure attention only flows to earlier positions\n",
    "        # Not a parameter, just a buffer\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                            .view(1, 1, config.block_size, config.block_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()  # Batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        \n",
    "        # Calculate Q, K, V for all heads in batch\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        # (B, T, C) -> (B, T, n_head, head_size) -> (B, n_head, T, head_size)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        \n",
    "        # Causal self-attention: (B, n_head, T, head_size) @ (B, n_head, head_size, T) -> (B, n_head, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        \n",
    "        # Apply causal mask\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        \n",
    "        # Apply attention to values: (B, n_head, T, T) @ (B, n_head, T, head_size) -> (B, n_head, T, head_size)\n",
    "        y = att @ v\n",
    "        \n",
    "        # Reassemble all head outputs side by side\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        # Output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "# Test the attention module\n",
    "test_attn = CausalSelfAttention(config).to(device)\n",
    "test_input = torch.randn(2, 10, config.n_embd).to(device)  # Batch=2, Seq=10\n",
    "test_output = test_attn(test_input)\n",
    "print(f\"Attention input shape: {test_input.shape}\")\n",
    "print(f\"Attention output shape: {test_output.shape}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in test_attn.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e843f22",
   "metadata": {},
   "source": [
    "## 5. MLP Block Implementation\n",
    "\n",
    "Create the feedforward network with GELU activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e1019a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP input shape: torch.Size([2, 10, 768])\n",
      "MLP output shape: torch.Size([2, 10, 768])\n",
      "Number of parameters: 4,722,432\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron (feedforward network).\n",
    "    Standard two-layer MLP with GELU activation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # First linear layer expands dimension by 4x\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        \n",
    "        # GELU activation (Gaussian Error Linear Unit)\n",
    "        # GPT-2 uses the approximate version\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        \n",
    "        # Second linear layer projects back to embedding dimension\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "# Test the MLP module\n",
    "test_mlp = MLP(config).to(device)\n",
    "test_input = torch.randn(2, 10, config.n_embd).to(device)\n",
    "test_output = test_mlp(test_input)\n",
    "print(f\"MLP input shape: {test_input.shape}\")\n",
    "print(f\"MLP output shape: {test_output.shape}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in test_mlp.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3478ca",
   "metadata": {},
   "source": [
    "## 6. Transformer Block Implementation\n",
    "\n",
    "Combine attention and MLP with pre-LayerNorm and residual connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49b80f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block input shape: torch.Size([2, 10, 768])\n",
      "Block output shape: torch.Size([2, 10, 768])\n",
      "Number of parameters: 7,087,872\n"
     ]
    }
   ],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer block with pre-LayerNorm architecture.\n",
    "    Contains self-attention and MLP with residual connections.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # LayerNorm before attention\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
    "        \n",
    "        # Multi-head self-attention\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        \n",
    "        # LayerNorm before MLP\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
    "        \n",
    "        # MLP feedforward\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Pre-LayerNorm architecture with residual connections\n",
    "        x = x + self.attn(self.ln_1(x))  # Attention block with residual\n",
    "        x = x + self.mlp(self.ln_2(x))   # MLP block with residual\n",
    "        return x\n",
    "\n",
    "# Test the transformer block\n",
    "test_block = Block(config).to(device)\n",
    "test_input = torch.randn(2, 10, config.n_embd).to(device)\n",
    "test_output = test_block(test_input)\n",
    "print(f\"Block input shape: {test_input.shape}\")\n",
    "print(f\"Block output shape: {test_output.shape}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in test_block.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de40b8cd",
   "metadata": {},
   "source": [
    "## 7. Complete GPT-2 Model Class\n",
    "\n",
    "Assemble all components into the full GPT-2 architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05066356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 123.65M\n",
      "\n",
      "Model created successfully!\n",
      "Total parameters: 123,653,376\n",
      "Non-embedding parameters: 123,653,376\n"
     ]
    }
   ],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\"\n",
    "    GPT-2 Language Model.\n",
    "    Full implementation matching Hugging Face's architecture.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "        \n",
    "        # Model architecture\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            # Token embeddings\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            \n",
    "            # Positional embeddings\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            \n",
    "            # Dropout on embeddings\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            \n",
    "            # Stack of transformer blocks\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            \n",
    "            # Final layer normalization\n",
    "            ln_f = nn.LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        \n",
    "        # Language model head (no bias, tied with token embeddings in original GPT-2)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        # Weight tying: share weights between token embeddings and output layer\n",
    "        # This reduces parameters and can improve performance\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        # Apply special scaled init to residual projections (GPT-2 paper)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "        \n",
    "        # Report number of parameters\n",
    "        print(f\"Number of parameters: {self.get_num_params()/1e6:.2f}M\")\n",
    "    \n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count, subtract position and token embeddings.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize weights following GPT-2 initialization scheme\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            idx: Token indices of shape (B, T)\n",
    "            targets: Optional target indices for computing loss (B, T)\n",
    "        \n",
    "        Returns:\n",
    "            logits: Output logits of shape (B, T, vocab_size)\n",
    "            loss: Cross-entropy loss if targets provided, else None\n",
    "        \"\"\"\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        \n",
    "        # Generate position indices [0, 1, 2, ..., t-1]\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)  # shape (t)\n",
    "        \n",
    "        # Token embeddings + positional embeddings\n",
    "        tok_emb = self.transformer.wte(idx)  # (B, T, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos)  # (T, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        \n",
    "        # Pass through all transformer blocks\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Final layer norm\n",
    "        x = self.transformer.ln_f(x)\n",
    "        \n",
    "        # Language model head to get logits\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "        \n",
    "        # Calculate loss if targets provided\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            # Flatten logits and targets for cross-entropy\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "# Create the model\n",
    "model = GPT(config)\n",
    "model.to(device)\n",
    "print(f\"\\nModel created successfully!\")\n",
    "print(f\"Total parameters: {model.get_num_params():,}\")\n",
    "print(f\"Non-embedding parameters: {model.get_num_params(non_embedding=True):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e617660",
   "metadata": {},
   "source": [
    "## 8. Weight Loading from Pretrained Model\n",
    "\n",
    "Load pretrained GPT-2 124M weights from Hugging Face into our custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a1427e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained weights from 'gpt2'...\n",
      "Weights loaded successfully!\n",
      "\n",
      "Model ready for inference!\n",
      "Weights loaded successfully!\n",
      "\n",
      "Model ready for inference!\n"
     ]
    }
   ],
   "source": [
    "def load_pretrained_weights(model, model_type='gpt2'):\n",
    "    \"\"\"\n",
    "    Load pretrained weights from Hugging Face GPT-2 into our custom model.\n",
    "    Handles weight transpositions and name mappings.\n",
    "    \"\"\"\n",
    "    print(f\"Loading pretrained weights from '{model_type}'...\")\n",
    "    \n",
    "    # Load Hugging Face model\n",
    "    model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "    sd_hf = model_hf.state_dict()\n",
    "    \n",
    "    # Get our model's state dict\n",
    "    sd = model.state_dict()\n",
    "    \n",
    "    # Copy weights, handling transpositions\n",
    "    sd_keys = sd.keys()\n",
    "    sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')]  # Ignore attention bias buffer\n",
    "    \n",
    "    # Map Hugging Face keys to our keys\n",
    "    # HF uses 'transformer.h.0.attn.c_attn.weight' format\n",
    "    # We use the same, so most keys match directly\n",
    "    \n",
    "    transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "    \n",
    "    for k in sd_keys:\n",
    "        # Check if this key needs transposition\n",
    "        # Conv1D layers in HF are stored transposed\n",
    "        if any(k.endswith(w) for w in transposed):\n",
    "            # Need to transpose\n",
    "            assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "            with torch.no_grad():\n",
    "                sd[k].copy_(sd_hf[k].t())\n",
    "        else:\n",
    "            # Direct copy\n",
    "            assert sd_hf[k].shape == sd[k].shape\n",
    "            with torch.no_grad():\n",
    "                sd[k].copy_(sd_hf[k])\n",
    "    \n",
    "    print(\"Weights loaded successfully!\")\n",
    "    return model\n",
    "\n",
    "# Load pretrained weights\n",
    "model = load_pretrained_weights(model, 'gpt2')\n",
    "model.eval()  # Set to evaluation mode\n",
    "print(\"\\nModel ready for inference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d57b455",
   "metadata": {},
   "source": [
    "## 9. Text Generation with Sampling\n",
    "\n",
    "Implement autoregressive text generation with top-k sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34d8fbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Hello, I'm a language model,\n",
      "\n",
      "Generated text:\n",
      "Hello, I'm a language model, and it is not easy to get things right.\n",
      "\n",
      "I also have a problem with doing many of their projects on the server. The client would have to wait for all the code to load before it could connect. It might be because I had\n",
      "Generated text:\n",
      "Hello, I'm a language model, and it is not easy to get things right.\n",
      "\n",
      "I also have a problem with doing many of their projects on the server. The client would have to wait for all the code to load before it could connect. It might be because I had\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "    \"\"\"\n",
    "    Generate new tokens autoregressively.\n",
    "    \n",
    "    Args:\n",
    "        model: The GPT model\n",
    "        idx: Starting token indices (B, T)\n",
    "        max_new_tokens: Number of tokens to generate\n",
    "        temperature: Sampling temperature (higher = more random)\n",
    "        top_k: If set, only sample from top k most likely tokens\n",
    "    \n",
    "    Returns:\n",
    "        Generated token indices (B, T + max_new_tokens)\n",
    "    \"\"\"\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Crop context if needed\n",
    "        idx_cond = idx if idx.size(1) <= config.block_size else idx[:, -config.block_size:]\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, _ = model(idx_cond)\n",
    "        \n",
    "        # Get logits for last token\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        \n",
    "        # Optionally apply top-k filtering\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Sample from the distribution\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        # Append sampled token\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    \n",
    "    return idx\n",
    "\n",
    "# Test generation\n",
    "prompt = \"Hello, I'm a language model,\"\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "# Encode prompt\n",
    "tokens = encode(prompt)\n",
    "tokens = torch.tensor(tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "# Generate\n",
    "generated_tokens = generate(model, tokens, max_new_tokens=50, temperature=0.8, top_k=50)\n",
    "\n",
    "# Decode\n",
    "generated_text = decode(generated_tokens[0].tolist())\n",
    "print(f\"Generated text:\\n{generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e119a83a",
   "metadata": {},
   "source": [
    "## 10. Dataset Preparation\n",
    "\n",
    "Prepare training data. We'll use a simple text file for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e4e3ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n",
      "Download complete!\n",
      "Dataset size: 1,115,394 characters\n",
      "Download complete!\n",
      "Dataset size: 1,115,394 characters\n",
      "Number of tokens: 338,025\n",
      "Train tokens: 304,222\n",
      "Val tokens: 33,803\n",
      "Number of tokens: 338,025\n",
      "Train tokens: 304,222\n",
      "Val tokens: 33,803\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Download tiny_shakespeare dataset for demonstration\n",
    "def download_dataset():\n",
    "    \"\"\"Download tiny_shakespeare dataset\"\"\"\n",
    "    url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    filename = 'input.txt'\n",
    "    \n",
    "    if not os.path.exists(filename):\n",
    "        print(\"Downloading dataset...\")\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        print(\"Download complete!\")\n",
    "    else:\n",
    "        print(\"Dataset already exists.\")\n",
    "    \n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Load and tokenize data\n",
    "text = download_dataset()\n",
    "print(f\"Dataset size: {len(text):,} characters\")\n",
    "\n",
    "# Tokenize entire dataset\n",
    "tokens = encode(text)\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "print(f\"Number of tokens: {len(tokens):,}\")\n",
    "\n",
    "# Split into train and validation\n",
    "n = int(0.9 * len(tokens))\n",
    "train_data = tokens[:n]\n",
    "val_data = tokens[n:]\n",
    "print(f\"Train tokens: {len(train_data):,}\")\n",
    "print(f\"Val tokens: {len(val_data):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60db69a2",
   "metadata": {},
   "source": [
    "## 11. DataLoader Implementation\n",
    "\n",
    "Create efficient batched data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "476d4922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape: torch.Size([4, 128])\n",
      "Target batch shape: torch.Size([4, 128])\n",
      "\n",
      "First sequence (first 10 tokens):\n",
      "Input: [286, 1918, 11, 198, 464, 10647, 286, 13795, 348, 2419]\n",
      "Target: [1918, 11, 198, 464, 10647, 286, 13795, 348, 2419, 314]\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split, batch_size=4, block_size=128):\n",
    "    \"\"\"\n",
    "    Generate a batch of data.\n",
    "    \n",
    "    Args:\n",
    "        split: 'train' or 'val'\n",
    "        batch_size: Number of sequences per batch\n",
    "        block_size: Length of each sequence\n",
    "    \n",
    "    Returns:\n",
    "        x: Input sequences (batch_size, block_size)\n",
    "        y: Target sequences (batch_size, block_size)\n",
    "    \"\"\"\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    # Randomly select starting indices\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    \n",
    "    # Extract sequences\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    \n",
    "    # Move to device\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "# Test the dataloader\n",
    "batch_size = 4\n",
    "block_size = 128\n",
    "xb, yb = get_batch('train', batch_size, block_size)\n",
    "print(f\"Input batch shape: {xb.shape}\")\n",
    "print(f\"Target batch shape: {yb.shape}\")\n",
    "print(f\"\\nFirst sequence (first 10 tokens):\")\n",
    "print(f\"Input: {xb[0, :10].tolist()}\")\n",
    "print(f\"Target: {yb[0, :10].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242d8d92",
   "metadata": {},
   "source": [
    "## 12. Loss Estimation\n",
    "\n",
    "Create a function to estimate loss on train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d669270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 4.7200\n",
      "Val loss: 4.6274\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters=200, batch_size=4, block_size=128):\n",
    "    \"\"\"\n",
    "    Estimate average loss on train and val sets.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to evaluate\n",
    "        eval_iters: Number of batches to average over\n",
    "        batch_size: Batch size\n",
    "        block_size: Sequence length\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with 'train' and 'val' losses\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    \n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, batch_size, block_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    \n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# Test loss estimation\n",
    "losses = estimate_loss(model, eval_iters=10, batch_size=4, block_size=128)\n",
    "print(f\"Train loss: {losses['train']:.4f}\")\n",
    "print(f\"Val loss: {losses['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c946f73c",
   "metadata": {},
   "source": [
    "## 13. Optimizer and Learning Rate Scheduler\n",
    "\n",
    "Set up AdamW optimizer with learning rate scheduling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba737812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fused AdamW: False\n",
      "Optimizer configured with learning rate: 0.0006\n"
     ]
    }
   ],
   "source": [
    "def configure_optimizers(model, weight_decay, learning_rate, betas, device_type):\n",
    "    \"\"\"\n",
    "    Configure optimizer with weight decay only on 2D parameters (matrices).\n",
    "    Following the GPT-2 paper, we don't apply weight decay to biases and LayerNorm.\n",
    "    \"\"\"\n",
    "    # Separate parameters into decay and no_decay groups\n",
    "    decay = set()\n",
    "    no_decay = set()\n",
    "    whitelist_weight_modules = (torch.nn.Linear, )\n",
    "    blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "    \n",
    "    for mn, m in model.named_modules():\n",
    "        for pn, p in m.named_parameters():\n",
    "            fpn = f'{mn}.{pn}' if mn else pn  # full param name\n",
    "            \n",
    "            if pn.endswith('bias'):\n",
    "                no_decay.add(fpn)\n",
    "            elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                decay.add(fpn)\n",
    "            elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                no_decay.add(fpn)\n",
    "    \n",
    "    # Validate that we considered every parameter\n",
    "    # Note: Due to weight tying (lm_head.weight = transformer.wte.weight),\n",
    "    # we need to only include parameters that actually exist in param_dict\n",
    "    param_dict = {pn: p for pn, p in model.named_parameters()}\n",
    "    \n",
    "    # Filter out parameter names that don't exist (due to weight tying)\n",
    "    decay = decay & param_dict.keys()\n",
    "    no_decay = no_decay & param_dict.keys()\n",
    "    \n",
    "    inter_params = decay & no_decay\n",
    "    union_params = decay | no_decay\n",
    "    assert len(inter_params) == 0, f\"parameters {inter_params} made it into both decay/no_decay sets!\"\n",
    "    assert len(param_dict.keys() - union_params) == 0, f\"parameters {param_dict.keys() - union_params} were not separated into either decay/no_decay set!\"\n",
    "    \n",
    "    # Create optimizer groups\n",
    "    optim_groups = [\n",
    "        {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": weight_decay},\n",
    "        {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    \n",
    "    # Use fused AdamW if available (faster on CUDA)\n",
    "    use_fused = (device_type == 'cuda') and ('fused' in torch.optim.AdamW.__init__.__code__.co_varnames)\n",
    "    print(f\"Using fused AdamW: {use_fused}\")\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, fused=use_fused)\n",
    "    return optimizer\n",
    "\n",
    "def get_lr(it, warmup_iters, lr_decay_iters, learning_rate, min_lr):\n",
    "    \"\"\"\n",
    "    Learning rate schedule with warmup and cosine decay.\n",
    "    \"\"\"\n",
    "    # 1) Linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) If it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) In between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "# Training hyperparameters (reduced for demonstration)\n",
    "learning_rate = 6e-4\n",
    "weight_decay = 1e-1\n",
    "betas = (0.9, 0.95)\n",
    "warmup_iters = 100\n",
    "lr_decay_iters = 5000\n",
    "min_lr = 6e-5\n",
    "\n",
    "# Create optimizer\n",
    "device_type = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "optimizer = configure_optimizers(model, weight_decay, learning_rate, betas, device_type)\n",
    "print(f\"Optimizer configured with learning rate: {learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd7989b",
   "metadata": {},
   "source": [
    "## 14. Training Loop with Mixed Precision\n",
    "\n",
    "Implement the main training loop with gradient accumulation and mixed precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffba07ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9866/3020302290.py:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "  Max iterations: 1000\n",
      "  Batch size: 4\n",
      "  Block size: 128\n",
      "  Gradient accumulation steps: 1\n",
      "  Mixed precision: False\n",
      "  Device: cpu\n",
      "\n",
      "Creating fresh model for training from scratch...\n",
      "Number of parameters: 123.65M\n",
      "Using fused AdamW: False\n",
      "Number of parameters: 123.65M\n",
      "Using fused AdamW: False\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "max_iters = 1000  # Reduced for demonstration\n",
    "eval_interval = 100\n",
    "eval_iters = 20\n",
    "batch_size = 4  # Reduced for demonstration\n",
    "block_size = 128  # Reduced for demonstration\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Mixed precision training\n",
    "use_amp = device_type == 'cuda'  # Use automatic mixed precision on CUDA\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Max iterations: {max_iters}\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Block size: {block_size}\")\n",
    "print(f\"  Gradient accumulation steps: {gradient_accumulation_steps}\")\n",
    "print(f\"  Mixed precision: {use_amp}\")\n",
    "print(f\"  Device: {device}\")\n",
    "print()\n",
    "\n",
    "# Note: For demonstration, we'll train a fresh model from scratch\n",
    "# If you want to finetune the pretrained model, skip the reinitialization\n",
    "print(\"Creating fresh model for training from scratch...\")\n",
    "train_config = GPTConfig()\n",
    "train_config.dropout = 0.1  # Add dropout for training\n",
    "train_model = GPT(train_config)\n",
    "train_model.to(device)\n",
    "\n",
    "# Recreate optimizer for the new model\n",
    "optimizer = configure_optimizers(train_model, weight_decay, learning_rate, betas, device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4ee48c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Note: Training from scratch takes significant time and compute.\n",
      "For demonstration, we're using reduced settings.\n",
      "For full GPT-2 reproduction, you'd need multiple GPUs and days of training.\n",
      "\n",
      "step 0: train loss 10.9815, val loss 10.9748, lr 0.000000\n",
      "step 0: train loss 10.9815, val loss 10.9748, lr 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9866/1012611959.py:24: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def train(model, optimizer, max_iters, eval_interval=100):\n",
    "    \"\"\"\n",
    "    Main training loop.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    for iter in range(max_iters):\n",
    "        # Determine learning rate for this iteration\n",
    "        lr = get_lr(iter, warmup_iters, lr_decay_iters, learning_rate, min_lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        # Evaluate at intervals\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "            losses = estimate_loss(model, eval_iters, batch_size, block_size)\n",
    "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, lr {lr:.6f}\")\n",
    "        \n",
    "        # Training step\n",
    "        for micro_step in range(gradient_accumulation_steps):\n",
    "            # Get batch\n",
    "            X, Y = get_batch('train', batch_size, block_size)\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "                logits, loss = model(X, Y)\n",
    "                loss = loss / gradient_accumulation_steps  # Scale loss for gradient accumulation\n",
    "            \n",
    "            # Backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "        \n",
    "        # Clip gradients\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # Optimizer step\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run training (uncomment to train)\n",
    "# This will take a while! For quick testing, set max_iters to a small number like 100\n",
    "print(\"Starting training...\")\n",
    "print(\"Note: Training from scratch takes significant time and compute.\")\n",
    "print(\"For demonstration, we're using reduced settings.\")\n",
    "print(\"For full GPT-2 reproduction, you'd need multiple GPUs and days of training.\")\n",
    "print()\n",
    "\n",
    "# Uncomment to actually train:\n",
    "train_model = train(train_model, optimizer, max_iters, eval_interval)\n",
    "\n",
    "print(\"Training loop defined. Uncomment the train() call to start training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcbab7d",
   "metadata": {},
   "source": [
    "## 15. Model Checkpointing\n",
    "\n",
    "Save and load model checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34027792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, iter, loss, filepath='checkpoint.pt'):\n",
    "    \"\"\"Save model checkpoint\"\"\"\n",
    "    checkpoint = {\n",
    "        'iter': iter,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        'config': model.config,\n",
    "    }\n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"Checkpoint saved to {filepath}\")\n",
    "\n",
    "def load_checkpoint(filepath='checkpoint.pt'):\n",
    "    \"\"\"Load model checkpoint\"\"\"\n",
    "    checkpoint = torch.load(filepath, map_location=device)\n",
    "    \n",
    "    # Recreate model\n",
    "    config = checkpoint['config']\n",
    "    model = GPT(config)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    \n",
    "    # Recreate optimizer\n",
    "    optimizer = configure_optimizers(model, weight_decay, learning_rate, betas, device_type)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    iter = checkpoint['iter']\n",
    "    loss = checkpoint['loss']\n",
    "    \n",
    "    print(f\"Checkpoint loaded from {filepath}\")\n",
    "    print(f\"Resuming from iteration {iter} with loss {loss:.4f}\")\n",
    "    \n",
    "    return model, optimizer, iter, loss\n",
    "\n",
    "# Example usage (uncomment to use):\n",
    "# save_checkpoint(model, optimizer, 0, 0.0, 'gpt2_checkpoint.pt')\n",
    "# loaded_model, loaded_optimizer, iter, loss = load_checkpoint('gpt2_checkpoint.pt')\n",
    "\n",
    "print(\"Checkpoint functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a93c604",
   "metadata": {},
   "source": [
    "## 16. Performance Optimization with torch.compile\n",
    "\n",
    "Use PyTorch 2.0's torch.compile for faster execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9302eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.compile is available in PyTorch 2.0+\n",
    "import sys\n",
    "\n",
    "# Check PyTorch version\n",
    "pytorch_version = torch.__version__\n",
    "print(f\"PyTorch version: {pytorch_version}\")\n",
    "\n",
    "# Compile model if PyTorch 2.0+ and CUDA available\n",
    "if sys.version_info >= (3, 8) and torch.__version__ >= '2.0' and device_type == 'cuda':\n",
    "    print(\"Compiling model with torch.compile()...\")\n",
    "    print(\"Note: First run will be slow due to compilation.\")\n",
    "    \n",
    "    # Compile the model\n",
    "    # This can provide 2-3x speedup on modern GPUs\n",
    "    compiled_model = torch.compile(model)\n",
    "    print(\"Model compiled successfully!\")\n",
    "    \n",
    "    # Benchmark\n",
    "    print(\"\\nBenchmarking compiled vs non-compiled model...\")\n",
    "    \n",
    "    # Warm up\n",
    "    X, Y = get_batch('train', batch_size=4, block_size=128)\n",
    "    _ = model(X, Y)\n",
    "    _ = compiled_model(X, Y)\n",
    "    \n",
    "    # Time non-compiled\n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        X, Y = get_batch('train', batch_size=4, block_size=128)\n",
    "        logits, loss = model(X, Y)\n",
    "    non_compiled_time = time.time() - start\n",
    "    \n",
    "    # Time compiled\n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        X, Y = get_batch('train', batch_size=4, block_size=128)\n",
    "        logits, loss = compiled_model(X, Y)\n",
    "    compiled_time = time.time() - start\n",
    "    \n",
    "    print(f\"Non-compiled: {non_compiled_time:.3f}s\")\n",
    "    print(f\"Compiled: {compiled_time:.3f}s\")\n",
    "    print(f\"Speedup: {non_compiled_time/compiled_time:.2f}x\")\n",
    "else:\n",
    "    print(\"torch.compile not available or not using CUDA. Skipping compilation.\")\n",
    "    print(\"To use torch.compile, you need:\")\n",
    "    print(\"  - PyTorch 2.0 or later\")\n",
    "    print(\"  - Python 3.8 or later\")\n",
    "    print(\"  - CUDA GPU (for best performance)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4da3e09",
   "metadata": {},
   "source": [
    "## 17. Generate Samples from Trained Model\n",
    "\n",
    "Generate multiple text samples with different prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbff5c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(model, prompts, max_new_tokens=100, temperature=0.8, top_k=50):\n",
    "    \"\"\"Generate text samples for multiple prompts\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    for i, prompt in enumerate(prompts):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Sample {i+1}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"{'-'*80}\")\n",
    "        \n",
    "        # Encode prompt\n",
    "        tokens = encode(prompt)\n",
    "        tokens = torch.tensor(tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            generated = generate(model, tokens, max_new_tokens, temperature, top_k)\n",
    "        \n",
    "        # Decode\n",
    "        text = decode(generated[0].tolist())\n",
    "        print(text)\n",
    "        print()\n",
    "\n",
    "# Sample prompts to test the model\n",
    "prompts = [\n",
    "    \"Hello, I'm a language model,\",\n",
    "    \"Once upon a time,\",\n",
    "    \"The meaning of life is\",\n",
    "    \"In the field of artificial intelligence,\",\n",
    "    \"ROMEO:\",\n",
    "]\n",
    "\n",
    "print(\"Generating samples from pretrained GPT-2 model...\")\n",
    "generate_samples(model, prompts, max_new_tokens=80, temperature=0.8, top_k=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158a6e8a",
   "metadata": {},
   "source": [
    "## 18. Model Evaluation and Metrics\n",
    "\n",
    "Calculate perplexity and other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a007b06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_model(model, num_batches=100):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation with multiple metrics.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for _ in range(num_batches):\n",
    "        X, Y = get_batch('val', batch_size=4, block_size=128)\n",
    "        logits, loss = model(X, Y)\n",
    "        \n",
    "        total_loss += loss.item() * Y.numel()\n",
    "        total_tokens += Y.numel()\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Model Evaluation Results\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Average loss: {avg_loss:.4f}\")\n",
    "    print(f\"Perplexity: {perplexity:.2f}\")\n",
    "    print(f\"Evaluated on {total_tokens:,} tokens\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    model.train()\n",
    "    return avg_loss, perplexity\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluating pretrained GPT-2 model on validation set...\")\n",
    "loss, perplexity = evaluate_model(model, num_batches=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7574e0d0",
   "metadata": {},
   "source": [
    "## 19. Summary and Next Steps\n",
    "\n",
    "Congratulations! You've built GPT-2 from scratch.\n",
    "\n",
    "### What We've Implemented:\n",
    "\n",
    "✅ **Architecture**:\n",
    "- Multi-head self-attention with causal masking\n",
    "- MLP feedforward blocks with GELU activation\n",
    "- Pre-LayerNorm transformer blocks\n",
    "- Token and positional embeddings\n",
    "- Full GPT-2 124M architecture\n",
    "\n",
    "✅ **Training Infrastructure**:\n",
    "- Efficient data loading\n",
    "- AdamW optimizer with learning rate scheduling\n",
    "- Mixed precision training\n",
    "- Gradient accumulation\n",
    "- Checkpointing\n",
    "\n",
    "✅ **Inference**:\n",
    "- Autoregressive text generation\n",
    "- Top-k sampling\n",
    "- Temperature scaling\n",
    "- Weight loading from Hugging Face\n",
    "\n",
    "✅ **Optimization**:\n",
    "- torch.compile support\n",
    "- Mixed precision (AMP)\n",
    "- Efficient batching\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Train from Scratch**: Uncomment the training loop and train on a larger dataset (OpenWebText, The Pile, etc.)\n",
    "\n",
    "2. **Scale Up**: \n",
    "   - Increase model size (GPT-2 medium/large/XL)\n",
    "   - Use larger batch sizes\n",
    "   - Train for more iterations\n",
    "\n",
    "3. **Advanced Features**:\n",
    "   - Implement Flash Attention for faster training\n",
    "   - Add distributed training (DDP, FSDP)\n",
    "   - Implement gradient checkpointing for memory efficiency\n",
    "\n",
    "4. **Fine-tuning**:\n",
    "   - Fine-tune on domain-specific data\n",
    "   - Implement instruction tuning\n",
    "   - Add RLHF (Reinforcement Learning from Human Feedback)\n",
    "\n",
    "5. **Evaluation**:\n",
    "   - Test on standard benchmarks (HellaSwag, MMLU, etc.)\n",
    "   - Compare with OpenAI's GPT-2\n",
    "   - Analyze generated samples\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- **Andrej Karpathy's Video**: [Let's reproduce GPT-2 (124M)](https://www.youtube.com/watch?v=l8pRSuU81PU)\n",
    "- **nanoGPT Repository**: https://github.com/karpathy/nanoGPT\n",
    "- **build-nanogpt Repository**: https://github.com/karpathy/build-nanogpt\n",
    "- **GPT-2 Paper**: [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "- **Attention Paper**: [Attention is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "Happy training! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
